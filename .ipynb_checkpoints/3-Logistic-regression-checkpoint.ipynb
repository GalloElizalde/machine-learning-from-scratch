{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression example\n",
    "\n",
    "We will use Logistic Regression to calculate the probability for a student to pass an exam given the number of hours of study and number of hours slept:\n",
    "\n",
    "$$p(\\text{passed}) = \\sigma(w_0 + w_1 \\cdot \\text{hours studied} + w_2 \\cdot \\text{hours slept})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize # for fit\n",
    "%matplotlib inline\n",
    "rng = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read data\n",
    "\n",
    "We first start to read a `cvs` file containing, for each student, the number of hours studied and slept (=the `features`) and the result (=the `labels`) of the exam: 0 (Failed) or 1 (passed).\n",
    "\n",
    "a) Separate the sample in student that passed the exam and student that failed the exam:\n",
    "```python\n",
    "labels = ...\n",
    "student_passed = ...\n",
    "student_failed = ...\n",
    "```\n",
    "\n",
    "b) Show on a figure for each student the number of hours slept as a function of the number of hours studied. Use a blue marker for students that passed and a red marker for those who failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "data/data_student_exams.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load and read file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m genfromtxt\n\u001b[0;32m----> 3\u001b[0m my_data \u001b[38;5;241m=\u001b[39m \u001b[43mgenfromtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/data_student_exams.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%10s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%10s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%8s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStudied\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSlept\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPassed\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Print first 10 entries\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/qc-env/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:1990\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[1;32m   1988\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fname)\n\u001b[1;32m   1989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1990\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1991\u001b[0m     fid_ctx \u001b[38;5;241m=\u001b[39m contextlib\u001b[38;5;241m.\u001b[39mclosing(fid)\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/qc-env/lib/python3.10/site-packages/numpy/lib/_datasource.py:192\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m \n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qc-env/lib/python3.10/site-packages/numpy/lib/_datasource.py:529\u001b[0m, in \u001b[0;36mDataSource.open\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    527\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: data/data_student_exams.csv not found."
     ]
    }
   ],
   "source": [
    "# Load and read file\n",
    "from numpy import genfromtxt\n",
    "my_data = genfromtxt('data/data_student_exams.csv', delimiter=',')\n",
    "print('%10s %10s %8s'% ('Studied', 'Slept', 'Passed'))\n",
    "\n",
    "# Print first 10 entries\n",
    "print(my_data[:10,:])\n",
    "\n",
    "# CONTINUE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Separate data in train and test sample\n",
    "\n",
    "a) You'll note below that we add a column of \"1\" to the vector of features ${\\bf x}$. Why do we do that ? The vector ${\\bf x}$ is redefined as ${\\bf x^*}$:\n",
    "$${\\bf x} \\rightarrow {\\bf x^*}$$\n",
    "\n",
    "b) Separate the sample in two: the first 500 entries for training and the remaining 500 entries for testing.\n",
    "```python\n",
    "features_train = ...\n",
    "labels_train = ...\n",
    "features_test = ...\n",
    "labels_test = ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data features: [1, studied, slept]\n",
    "features = my_data[:,0:2]\n",
    "bias = np.ones(shape=(len(features),1))\n",
    "features = np.append(bias, features, axis=1)\n",
    "\n",
    "# N examples for train and test\n",
    "N=500\n",
    "\n",
    "# CONTINUE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sigmoid function\n",
    "\n",
    "For training the Logistic Regression we'll need the sigmoid function and its derivative\n",
    "\n",
    "a) Write two functions, `sigmoid(x)` and `dsig(x)` that returns, respectively $\\sigma(x)$ and $\\frac{\\text{d} \\sigma}{\\text{d} x}(x)$.\n",
    "\n",
    "b) Represent graphically both functions on the same figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building a predictive model\n",
    "\n",
    "The predictive model for a feature ${\\bf x}$ is given by the function:\n",
    "\n",
    "$$f({\\bf x}) = \\sigma \\left( w_0 + \\sum_{i=1}^D w_i x_i \\right) = \\sigma \\left(\\sum_{i=0}^D w_i x^*_i \\right)$$\n",
    "\n",
    "The cost function used in the Logistic Regression is the cross-entropy:\n",
    "\n",
    "$$E({\\bf w}) = - \\frac{1}{N}\\sum_{j=1}^N t_j \\log( f({\\bf x_j})) + (1-t_j) \\log( 1 - f({\\bf x_j}))$$\n",
    "\n",
    "The gradient of the cross-entropy function is:\n",
    "$$\n",
    "\\vec{\\nabla} E({\\bf w}) = \\frac{1}{N}\\sum_{j=1}^N \\left[ f({\\bf x_j}) - t_j \\right] {\\bf x^*_j} \\rightarrow\n",
    "\\begin{cases}\n",
    "\\frac{\\partial E({\\bf w})}{\\partial w_0} = \\frac{1}{N}\\sum_{j=1}^N \\left[ f({\\bf x_j}) - t_j \\right] \\\\\\\\\n",
    "\\frac{\\partial E({\\bf w})}{\\partial w_1} = \\frac{1}{N}\\sum_{j=1}^N \\left[ f({\\bf x_j}) - t_j \\right] x_{j1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial E({\\bf w})}{\\partial w_D} = \\frac{1}{N}\\sum_{j=1}^N \\left[ f({\\bf x_j}) - t_j \\right] x_{jD}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions needed for training\n",
    "\n",
    "Let's define the functions needed to train the algorithm. Look at all the functions below and explain for each what they do.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(features, weights):\n",
    "    '''\n",
    "    Returns 1D array of probabilities\n",
    "    that the class label == 1\n",
    "    '''\n",
    "    z = np.dot(features, weights)\n",
    "    return sigmoid(z)\n",
    "\n",
    "def cost_function(features, labels, weights):\n",
    "    '''\n",
    "    Cross-Entropy cost function\n",
    "\n",
    "    Features:(100,3)\n",
    "    Labels: (100,1)\n",
    "    Weights:(3,1)\n",
    "    Returns 1D matrix of predictions\n",
    "    Cost = sum ( labels*log(predictions) + (1-labels)*log(1-predictions) ) / len(labels)\n",
    "    '''\n",
    "    observations = len(labels)\n",
    "\n",
    "    predictions = predict(features, weights)\n",
    "\n",
    "    #Take the error when label=1\n",
    "    class1_cost = -labels*np.log(predictions)\n",
    "\n",
    "    #Take the error when label=0\n",
    "    class2_cost = (1-labels)*np.log(1-predictions)\n",
    "\n",
    "    #Take the sum of both costs\n",
    "    cost = class1_cost - class2_cost\n",
    "\n",
    "    #Take the average cost\n",
    "    cost = cost.sum()/observations\n",
    "\n",
    "    return cost\n",
    "\n",
    "def update_weights(features, labels, weights, lr):\n",
    "    '''\n",
    "    Gradient Descent\n",
    "\n",
    "    Features:(100, 3)\n",
    "    Labels: (100, 1)\n",
    "    Weights:(3, 1)\n",
    "    '''\n",
    "    N = len(features)\n",
    "\n",
    "    #1 - Get Predictions\n",
    "    predictions = predict(features, weights)\n",
    "\n",
    "    #2 Transpose features from (100, 3) to (3, 100)\n",
    "    # So we can multiply w the (100,1)  cost matrix.\n",
    "    # Returns a (3,1) matrix holding 3 partial derivatives --\n",
    "    # one for each feature -- representing the aggregate\n",
    "    # slope of the cost function across all observations\n",
    "    gradient = np.dot(features.T,  predictions - labels)\n",
    "    \n",
    "    #3 Take the average cost derivative for each feature\n",
    "    gradient /= N\n",
    "\n",
    "    #4 - Multiply the gradient by our learning rate\n",
    "    gradient *= lr\n",
    "    \n",
    "    #5 - Subtract from our weights to minimize cost\n",
    "    weights -= gradient\n",
    "\n",
    "    return weights\n",
    "\n",
    "def train(features, labels, weights, lr, iters):\n",
    "    \"\"\"\n",
    "    Training using gradient descent\n",
    "    \"\"\"\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(iters):\n",
    "        weights = update_weights(features, labels, weights, lr)\n",
    "\n",
    "        #Calculate error for auditing purposes\n",
    "        cost = cost_function(features, labels, weights)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Log Progress\n",
    "        #if i % 1000 == 0:\n",
    "        #    print(\"iter: \"+str(i) + \" cost: \"+str(cost))\n",
    "\n",
    "    return weights, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "Calculate the weights with gradient descent for 20000 steps. Plot cost function evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "weights = np.array([0.0,0.0,0.0])\n",
    "\n",
    "# Calculate weights with gradient descent\n",
    "Niter=20000\n",
    "learning_speed = 0.1\n",
    "\n",
    "# CONTINUE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Accuracy of predictions\n",
    "\n",
    "a) Determine the predicted probabilities for the training dataset. Deduce the predicted labels (0 or 1).\n",
    "\n",
    "b) Calculate how correct predictions are: for this you can compare the predicted labels to true labels and divide by the total number of lables. \n",
    "\n",
    "c) Same questions for the test dataset. Is there any overtraining ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Show results (with probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Show on a figure for each student the number of hours slept as a function of the number of hours studied. Use markers with a color depending on the probability value of the student to pass the exam: from dark red for low probabilities to dark blue for high probabilities.\n",
    "\n",
    "b) Add a line showing the decision boundary separing both classes.  Hint: this line correspond to points for which p(accepted)=0.5, that is, students for which the weighted sum is such that $(w_0 + w_1 \\cdot x_1 + w_2 \\cdot x_2)=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Predicted probability distributions\n",
    "\n",
    "Show the distribution of predicted probabilities for student that passed or failed the exam, for both train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Confusion matrix\n",
    "\n",
    "See below how to construct a confusion matrix. \n",
    "\n",
    "The `threshold` value corresponds to the probability threshold over which student are labelled as Passing the exam.\n",
    "\n",
    "Change the threshold value and see how the values in the confusion matrix change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "#print(sns.__version__)\n",
    "\n",
    "LABELS = [\"Fail\",\"Pass\"]\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "y_pred = [1 if e > threshold else 0 for e in predicted_probabilities_test]\n",
    "conf_matrix = confusion_matrix(labels_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show()\n",
    "\n",
    "TN = conf_matrix[0,0] # True Negative (Fail -> Fail)\n",
    "FP = conf_matrix[0,1] # False Positive (Fail -> Pass)\n",
    "FN = conf_matrix[1,0] # False Negative (Pass -> Fail)\n",
    "TP = conf_matrix[1,1] # True Positive (Pass -> Pass)\n",
    "print('False positive rate = %.2f %%' % (FP/(FP+TN)*100))\n",
    "print('True positive rate = %.2f %%' % (TP/(TP+FN)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve\n",
    "\n",
    "A more global way to assess the performance of the algorithm is to construct the ROC curve (\"Receiver Operating Characteristic\"). The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n",
    "\n",
    "Look at the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "#target = np.concatenate((np.zeros(x_train.shape[0]),np.ones(x_test.shape[0])))\n",
    "#scores = np.concatenate((norm_train,norm_test))  \n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "# Train set                   \n",
    "fp, vp, thresholds = roc_curve(labels_train,predicted_probabilities_train,pos_label=1,drop_intermediate='False')\n",
    "roc_auc = auc(fp, vp)\n",
    "plt.plot(fp,vp,color='red',label='ROC curve Train (AUC = %0.4f)'%(roc_auc))\n",
    "# Test set                   \n",
    "fp, vp, thresholds = roc_curve(labels_test,predicted_probabilities_test,pos_label=1,drop_intermediate='False')\n",
    "roc_auc = auc(fp, vp)\n",
    "plt.plot(fp,vp,color='green',label='ROC curve Test (AUC = %0.4f)'%(roc_auc))\n",
    "\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.plot([0, 1],[0, 1],linestyle='--',color=(0.6, 0.6, 0.6),label='Random guess')\n",
    "plt.grid()\n",
    "plt.legend(loc=\"best\",fontsize=16)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"images/ROC.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
